{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d582815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "#os.environ['OPENAI_API_KEY'] = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f26261",
   "metadata": {},
   "source": [
    "## 1. Agent 설정\n",
    "LLM, Temperature, 프롬프트, 페르소나 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef47ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "                    temperature=0,\n",
    "                    request_timeout=60,\n",
    "                    api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "system_prompt = '''Y\n",
    "You are a logical reasoning assistant.  \n",
    "You will be shown a short story as a list of sentences and then a question.  \n",
    "Your task is to answer using exactly one English word naming a location (e.g., “desk”, “bookshelf”).  \n",
    "Do not add any extra words, punctuation, or explanation—just the single location word.\n",
    "'''\n",
    "\n",
    "integrated_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt.strip()),\n",
    "    (\"human\", \"\"\"\n",
    "[Story]\n",
    "{Story}\n",
    "\n",
    "[Question]\n",
    "{Question}\n",
    "\n",
    "\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17556a",
   "metadata": {},
   "source": [
    "## 1. 질문 하나만 테스트 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8be4725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = '''\n",
    "1. Oliver entered the porch.\n",
    "2. Owen entered the porch.\n",
    "3. Abigail entered the porch.\n",
    "4. The broccoli is in the bucket.\n",
    "5. Abigail exited the porch.\n",
    "6. Owen exited the porch.\n",
    "7. Owen hates the peach\n",
    "8. Oliver moved the broccoli to the pantry.\n",
    "'''\n",
    "qa_list = [\n",
    "    {'type': 'reality', 'question':'Where is the broccoli really?', 'answer':'pantry'},\n",
    "    {'type':'memory', 'question':'Where was the broccoli at the beginning?', 'answer':'bucket'},\n",
    "    {'type':'first_order', 'question':'Where will Oliver look for the broccoli?', 'answer':'pantry'},\n",
    "    {'type':'first_order', 'question':'Where will Abigail look for the broccoli?', 'answer':'bucket'},\n",
    "    {'type':'second_order', 'question':'Where does Oliver think that Abigail searches for the broccoli?', 'answer':'pantry'},\n",
    "    {'type':'second_order', 'question':'Where does Abigail think that Oliver searches for the broccoli?', 'answer':'pantry'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a6737d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mQuestion 1 (reality): )Where is the broccoli really?\u001b[0m\n",
      "  Model answer: pantry, Correct answer: pantry\n",
      "\u001b[94mQuestion 2 (memory): )Where was the broccoli at the beginning?\u001b[0m\n",
      "  Model answer: bucket, Correct answer: bucket\n",
      "\u001b[94mQuestion 3 (first_order): )Where will Oliver look for the broccoli?\u001b[0m\n",
      "  Model answer: pantry, Correct answer: pantry\n",
      "\u001b[94mQuestion 4 (first_order): )Where will Abigail look for the broccoli?\u001b[0m\n",
      "  Model answer: pantry, Correct answer: bucket\n",
      "\u001b[94mQuestion 5 (second_order): )Where does Oliver think that Abigail searches for the broccoli?\u001b[0m\n",
      "  Model answer: pantry, Correct answer: pantry\n",
      "\u001b[94mQuestion 6 (second_order): )Where does Abigail think that Oliver searches for the broccoli?\u001b[0m\n",
      "  Model answer: pantry, Correct answer: pantry\n"
     ]
    }
   ],
   "source": [
    "for i, qa in enumerate(qa_list, 1):\n",
    "    response = llm.invoke(integrated_prompt.format_messages(\n",
    "        Story=story.strip(), Question=qa['question']\n",
    "    ))\n",
    "    answer = response.content.strip()\n",
    "    print(f\"\\033[94mQuestion {i} ({qa['type']}): ){qa['question']}\\033[0m\")\n",
    "    print(f\"  Model answer: {answer}, Correct answer: {qa['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
